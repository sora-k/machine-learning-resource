<!DOCTYPE html>
<html>
<body>

<nav>
  <ul>
    <li><a href="index.html">Home</a></li>
    <li><a href="about.html">About Us</a></li>
    <li><strong>Bayesian Statistics</strong></li>
  </ul>
</nav>

<section>
	<h2>Bayes theorem</h2>
	<p>
		<ul>
			<li>P(A and B) = P(A)P(B|A) = P(B)P(A|B)
			<li>P(A|B) = P(A)P(B|A) / P(B)
			<li>P(Hypothesis | Evidence) = P(Hypothesis)P(Evidence | Hypothesis) / P(Evidence)
			<li>(posterior) = (prior)(likelyhood) / (normalizing constant)
			<li>P(A|B) = P(A and B) / P(B) = P(B|A) ( P(A) / P(B) ) - [4 slide 4]</li>
			<li>In words:
				<ul>
					<li> The conditional probability of A given B is the conditional probability
					     of B given A "scaled" by the relative probability of A compared to B.
					</li>
				</ul>
			</li>
			<li>Example: 
				<ul>	
					<li>P(Near ocean | Picked up seashell) = P( Picked up seashell | Near ocean) ( P(Near Ocean) / P(Picked up seashell) )  - [4 slide 7]</li>
					<li>This is rougly ~ (# times picked up seashell "at" the ocean) / (total seashell picked up)</li>
				</ul>
			</li>
		</ul>
	</p>
</section>
<section>
	<h2>Probability Mass Function (PMF)</h2>
	<p>
		<ul>
			<li>A map from "values" to "probabilities". </li>
			<li>For example: rolling dice 
				<ul>
					<li>value -> probability
					<li>1 -> 1/6
					<li>2 -> 1/6
					<li>3 -> 1/6
					<li>4 -> 1/6
					<li>5 -> 1/6
					<li>6 -> 1/6
				</ul>
		</ul>
	</p>
</section>
<section>
	<h2>Histogram</h2>
	<p>
		<ul>
			<li>A map from "values" to "frequencies". </li>
		</ul>
	</p>
</section>
<section>
	<h2>Confidence</h2>
	<p>
		<ul>
			<li>At the beginning, without any data, lets assume the prior is a uniform distribution.</li>
			<li>As evidence comes in, update prior confidence as data comes in.</li>
			<li>For example, determining the probability of getting a head flipping a euro coin. 
				<ul>
					<li>Let's say everyone in this room has their own prior assumption.</li>
					<li>"When you all start out you are all going to be equally confidence of your hypytheses. <br> Let's say there's a thousand points of confidence. (Meaning, each individual hypothesis has a thousand units of confidence.) Everyone gives their hypothesis a thousand point. "</li>
					<li>"Let's say, the first flip of the coin is head. People with the hypothesis of P(coin-flip = head) < 5 percent,  will update and lower their confidence points."</li>
				</ul> </li>
			<li> P(H|E) = P(H)P(E|H) / P(E) </li>
			<li> Description: 
				<ul>
					<li> "Belief about that hypothesis having seen that evidence is going to be proportional to how much did you believe it before, how likely is the evidence if you were right and over how likely is the evidence over all the circumstances at all."
					</li>
				</ul>
			<li> P(H|E) = P(H)P(E|H) : Skip the calcuation of P(E) until the end.</li>
				<ul>
					<li>(conf. after each coin toss) = (conf. before)  * (probability of evidence if your hypothesis were right)
				</ul>
			<li>Let's say, Evidence = [H, H, T] -> initiali confidence of 1000 for p of 0.25 => conf. = 1000 * (0.25)(0.25)(1 - 0.25)</li>	
		</ul>
	</p>
</section>
<footer>
	<h2>Reference</h3>
	<ol>
		<li><a href="https://www.youtube.com/watch?v=bobeo5kFz1g">Bayesian statistics made (as) simple (as possible)</a>--> <a href="https://sites.google.com/site/simplebayes/home/pycon-2012">slide/code</a></li>
		<li><a href="https://github.com/sora-k/machine-learning-resource/tree/master/bayesian-statistics/bayes_tutorial">code in repo</a></li>
		<li><a href="http://www.inference.phy.cam.ac.uk/itprnn/book.pdf">MacKay, Information Theory, Inference, and Learning Algorithms</a></li>
		<li><a href="http://faculty.washington.edu/kenrice/BayesIntroClassEpi515.pdf">kenrice-BayesIntroClassEpi515</a></li>
	</ol>
</footer>
</body>
</html>
